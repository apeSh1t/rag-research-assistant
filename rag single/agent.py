"""
ä½¿ç”¨Agentæ¶æ„ï¼Œè‡ªä¸»æœç´¢çŸ¥è¯†åº“å¹¶ç”Ÿæˆè¯¦ç»†è§£å†³æ–¹æ¡ˆ
"""
import json
from typing import Dict, List, Any
from langchain.tools import tool
from langchain.agents import create_openai_tools_agent, AgentExecutor
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from knowledge_base.kb import KnowledgeBase


class Agent:
    """è§„åˆ’Agentï¼šåŸºäºçŸ¥è¯†åº“å’Œå·¥å…·æ¥å£json schemaï¼Œç”Ÿæˆåˆ†æ­¥ã€ç»“æ„åŒ–çš„è§£å†³æ–¹æ¡ˆè®¡åˆ’"""
    
    def __init__(self, knowledge_base: KnowledgeBase, tools_schema_path: str = None, model_name: str = "qwen-max", api_key: str = None, base_url: str = None):
        self.kb = knowledge_base
        
        # é»˜è®¤åœ¨å½“å‰æ–‡ä»¶æ‰€åœ¨ç›®å½•æŸ¥æ‰¾ tools_schema.json
        if tools_schema_path is None:
            import os
            current_dir = os.path.dirname(os.path.abspath(__file__))
            tools_schema_path = os.path.join(current_dir, "tools_schema.json")
            
        self.tools_schema = self._load_tools_schema(tools_schema_path)
        
        # åˆ›å»ºçŸ¥è¯†åº“æœç´¢å·¥å…·
        @tool
        def search_knowledge(query: str) -> str:
            """
            Search the knowledge base for relevant information.
            """
            return self.kb.retrieve(query, k=3)
        

        self.tools = [search_knowledge]

        # åˆå§‹åŒ–LLMå’ŒAgent
        self.llm = ChatOpenAI(
            model=model_name,
            temperature=0.1,
            api_key=api_key or "your-api-key-here",
            base_url=base_url or "https://dashscope.aliyuncs.com/compatible-mode/v1"
        )
        
        # è®¾ç½®è§„åˆ’Agent
        self._setup_planning_agent()
    
    def _load_tools_schema(self, schema_path: str) -> List[Dict]:
        """åŠ è½½å·¥å…·schema"""
        try:
            with open(schema_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"è­¦å‘Šï¼šæ— æ³•åŠ è½½å·¥å…·schema ({e})ï¼Œå°†ä½¿ç”¨ç©ºschema")
            return []
    
    def _setup_planning_agent(self):
        """è®¾ç½®è§„åˆ’Agent"""   
        system_prompt = f"""You are a solution planning expert. Your task is to analyze user problems and formulate detailed step-by-step solution plans. Simple problems can be solved directly by LLM calculation, while complex problems require querying the knowledge base and calling tools.

IMPORTANT: You must ALWAYS output in ENGLISH, regardless of the user's input language.

Available Tools:
- search_knowledge: Query the knowledge base for relevant methods and principles.

Workflow:
1. Encountering unknown problems or concepts â†’ Call search_knowledge to query the knowledge base.
2. If search yields no results, try analyzing the problem and changing the query to search again. You must make decisions based on knowledge, and search at least once.
3. Based on the returned information, formulate a detailed execution plan:
   - If the knowledge base returns a method description â†’ Understand it and if it can be solved directly by LLM calculation -- add an llm_reasoning step; if still unclear, continue querying the knowledge base.
4. Based on the expected return of the tool or llm_reasoning, continue to analyze whether the goal is achieved and what to do next.
5. Recursively handle all sub-problems until the user's problem can be completely solved and the desired result is obtained.
6. Before outputting, analyze the plan to assess if every step is clear, if it solves the user's problem, and if the final output is simple and easy to understand. Otherwise, repeat the steps above.
7. When you get the final result, you MUST start your final answer with "Final Answer:".

Please clearly describe your every action during the thinking process, for example: "I will first search for knowledge about...", "Based on the search results, I found..., next I will...".

Example:
User: "Prepare RGB(128,20,190) color"
â†’ search_knowledge("RGB color preparation")
â† Returns: Needs RGBâ†’CMY, ratio calculation, multi-component mixing
â†’ search_knowledge("RGB to CMY")
â† Returns: Formula C=255-R...
â†’ Add direct calculation step
â†’ Analyze next step "Calculate ratio"
"""

        # åˆ›å»ºå·¥å…·åˆ—è¡¨
        tools = self.tools
        
        # åˆ›å»ºæç¤ºæ¨¡æ¿
        prompt = ChatPromptTemplate.from_messages([
            ("system", system_prompt),
            ("human", "{input}"),
            ("placeholder", "{agent_scratchpad}")
        ])
        
        # åˆ›å»ºAgent
        agent = create_openai_tools_agent(
            llm=self.llm,
            tools=tools,
            prompt=prompt
        )
        
        # åˆ›å»ºAgentæ‰§è¡Œå™¨
        self.agent_executor = AgentExecutor(
            agent=agent,
            tools=tools,
            verbose=True,
            max_iterations=10,
            handle_parsing_errors=True
        )
       
    def run(self, user_input: str) -> Dict[str, Any]:
        """
        æ ¹æ®ç”¨æˆ·è¾“å…¥åˆ›å»ºè¯¦ç»†çš„è§£å†³æ–¹æ¡ˆè®¡åˆ’
        ä½¿ç”¨Agentæ¶æ„ï¼Œè®©LLMè‡ªä¸»å†³å®šä½•æ—¶æœç´¢çŸ¥è¯†åº“
        """
        try:
            # ä½¿ç”¨Agentæ‰§è¡Œå™¨å¤„ç†ç”¨æˆ·è¾“å…¥
            result = self.agent_executor.invoke({
                "input": f"Please formulate a detailed solution plan for the following problem:\n\n{user_input}"
            })
            return result
        except Exception as e:
            return {"success": False, "error": str(e)}

    async def run_stream(self, user_input: str):
        """
        å¼‚æ­¥æµå¼è¾“å‡º Agent çš„æ€è€ƒè¿‡ç¨‹å’Œç»“æœ
        """
        # ä¿æŒä¸ run æ–¹æ³•ä¸€è‡´çš„ prompt æ„å»º
        full_input = f"Please formulate a detailed solution plan for the following problem:\n\n{user_input}"
        
        try:
            # è®°å½•å®Œæ•´çš„æ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆç­”æ¡ˆ
            buffer = ""
            final_answer_started = False
            action_started = False
            
            async for event in self.agent_executor.astream_events(
                {"input": full_input},
                version="v1"
            ):
                kind = event["event"]
                
                # æ•è·å·¥å…·è°ƒç”¨å¼€å§‹
                if kind == "on_tool_start":
                    # å¦‚æœæœ‰æœªå‘é€çš„ç¼“å†²å†…å®¹ï¼ˆé€šå¸¸æ˜¯æ€è€ƒï¼‰ï¼Œå…ˆå‘é€
                    if buffer.strip() and not action_started:
                        # æ¸…ç† "Thought:" å‰ç¼€
                        content_to_send = buffer.replace("Thought:", "").replace("Thought", "").strip()
                        if content_to_send:
                            yield {"type": "thought_chunk", "content": content_to_send}
                    buffer = ""
                    
                    action = event['data'].get('input')
                    # å…¼å®¹ä¸åŒç‰ˆæœ¬çš„ LangChain
                    if hasattr(action, 'tool'):
                        tool_name = action.tool
                        tool_input = action.tool_input
                    else:
                        tool_name = event['name']
                        tool_input = event['data'].get('input')
                        
                    yield {
                        "type": "thought",
                        "content": f"Using {tool_name}...",
                        "tool": tool_name,
                        "tool_input": tool_input
                    }
                
                # æ•è·æ¨¡å‹è¾“å‡ºçš„ Token (å®ç°æ‰“å­—æœºæ•ˆæœ)
                elif kind == "on_chat_model_stream":
                    content = event["data"]["chunk"].content
                    if not content:
                        continue
                    
                    # 1. å¦‚æœå·²ç»è¿›å…¥ Final Answer é˜¶æ®µï¼Œç›´æ¥ä½œä¸ºå›ç­”å‘é€
                    if final_answer_started:
                        yield {"type": "answer_chunk", "content": content}
                        continue

                    # 2. å¦‚æœå·²ç»è¿›å…¥ Action é˜¶æ®µï¼ˆæ­£åœ¨ç”Ÿæˆå·¥å…·è°ƒç”¨ä»£ç ï¼‰ï¼Œåˆ™ä¸å‘é€ç»™å‰ç«¯ï¼ˆéšè— Action å£°æ˜ï¼‰
                    if action_started:
                        continue

                    # 3. ç¼“å†²å†…å®¹ä»¥è¿›è¡Œæ£€æµ‹
                    buffer += content
                    
                    # æ£€æµ‹ Final Answer
                    if "Final Answer" in buffer:
                        final_answer_started = True
                        # æ‰¾åˆ°åˆ†å‰²ç‚¹
                        split_marker = "Final Answer:" if "Final Answer:" in buffer else "Final Answer"
                        parts = buffer.split(split_marker, 1)
                        
                        # åˆ†å‰²ç‚¹ä¹‹å‰çš„å†…å®¹å±äºæ€è€ƒ
                        thought_content = parts[0].replace("Thought:", "").replace("Thought", "").strip()
                        if thought_content:
                            yield {"type": "thought_chunk", "content": thought_content}
                        
                        # åˆ†å‰²ç‚¹ä¹‹åçš„å†…å®¹å±äºå›ç­”
                        if len(parts) > 1 and parts[1]:
                            yield {"type": "answer_chunk", "content": parts[1]}
                        
                        buffer = "" # æ¸…ç©ºç¼“å†²
                        continue

                    # æ£€æµ‹ Action (éšè— Action: ... åŠå…¶åçš„å†…å®¹ç›´åˆ°å·¥å…·è°ƒç”¨)
                    if "Action" in buffer:
                        # æ‰¾åˆ° Action çš„ä½ç½®
                        action_index = buffer.find("Action")
                        # Content before Action is thought
                        thought_content = buffer[:action_index].replace("Thought:", "").replace("Thought", "").strip()
                        if thought_content:
                            yield {"type": "thought_chunk", "content": thought_content}
                        
                        action_started = True
                        buffer = "" # æ¸…ç©ºç¼“å†²ï¼Œåç»­çš„ Action å†…å®¹å°†è¢«å¿½ç•¥
                        continue

                    # å¦‚æœç¼“å†²åŒºè¿‡å¤§ä¸”æ²¡æœ‰ç‰¹æ®Šæ ‡è®°ï¼Œåˆ™å°†å‰é¢çš„å†…å®¹ä½œä¸ºæ€è€ƒå‘é€
                    # ä¿ç•™æœ€åä¸€éƒ¨åˆ†ä»¥é˜²æ ‡è®°è¢«åˆ‡æ–­ (ä¾‹å¦‚ "Final A" æˆ– "Act")
                    if len(buffer) > 20:
                        to_send = buffer[:-15]
                        buffer = buffer[-15:]
                        # åªæœ‰å½“ to_send ä¸ä»…ä»…æ˜¯ "Thought" æ—¶æ‰å‘é€ï¼Œé¿å…é‡å¤
                        clean_send = to_send.replace("Thought:", "").replace("Thought", "").strip()
                        if clean_send:
                            yield {"type": "thought_chunk", "content": clean_send}
                
                # æ•è·å·¥å…·æ‰§è¡Œç»“æŸ
                elif kind == "on_tool_end":
                    action_started = False # Action ç»“æŸï¼Œæ¢å¤æ­£å¸¸æµå¼è¾“å‡ºï¼ˆé€šå¸¸æ¥ä¸‹æ¥æ˜¯ Observationï¼‰
                    buffer = "" # ç¡®ä¿ç¼“å†²æ¸…ç©º
                        
                    output = event['data'].get('output')
                    yield {
                        "type": "observation",
                        "content": str(output) if output else "No result",
                        "tool": event['name']
                    }
                
                # æ•è·æœ€ç»ˆè¾“å‡º
                elif kind == "on_agent_finish":
                    # ç¡®ä¿æ‰€æœ‰ç¼“å†²éƒ½å·²å¤„ç†
                    if buffer and not final_answer_started and not action_started:
                         # å¦‚æœç¼“å†²åŒºåŒ…å« Final Answerï¼Œè¯´æ˜å®ƒæ˜¯å›ç­”çš„ä¸€éƒ¨åˆ†ï¼Œä¸è¦ä½œä¸ºæ€è€ƒå‘é€
                         if "Final Answer" in buffer:
                             pass
                         else:
                             clean_buffer = buffer.replace("Thought:", "").replace("Thought", "").strip()
                             if clean_buffer:
                                yield {"type": "thought_chunk", "content": clean_buffer}
                    
                    output = event["data"]["output"]
                    # æ¸…ç† Final Answer æ ‡è®°ï¼Œé˜²æ­¢é‡å¤
                    clean_output = output.replace("Final Answer:", "").replace("Final Answer", "").strip()
                    yield {
                        "type": "final_answer",
                        "content": clean_output
                    }
        except Exception as e:
            yield {"type": "error", "content": str(e)}
    
def test_agent():
    """æµ‹è¯•è§„åˆ’Agent"""
    # åˆå§‹åŒ–çŸ¥è¯†åº“å’Œè§„åˆ’Agent
    kb = KnowledgeBase("knowledge_base")
    agent = Agent(kb)
    
    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        {
            "name": "Alcohol Dilution Test",
            "input": "I need to dilute 95% alcohol to 70%"
        },
        {
            "name": "Multi-component Mixing Test", 
            "input": "I need to prepare a mixture containing 0.15 concentration NaCl and 0.25 concentration glucose. I have pure water, 30% NaCl solution, and 60% glucose solution."
        },
        {
            "name": "Color Preparation Test",
            "input": "Prepare RGB(150,20,190) color, k=0.6"
        }
    ]
    
    for i, test_case in enumerate(test_cases, 1):
        print(f"\n{'='*80}")
        print(f"ğŸ¯ Planning Test {i}: {test_case['name']}")
        print(f"{'='*80}")
        print(f"Problem: {test_case['input']}")
        print("-" * 80)
        
        # Call
        result = agent.run(test_case['input'])
        print(result)
        print("-" * 80)
        input("\næŒ‰Enterç»§ç»­ä¸‹ä¸€ä¸ªæµ‹è¯•...")


if __name__ == "__main__":
    test_agent()