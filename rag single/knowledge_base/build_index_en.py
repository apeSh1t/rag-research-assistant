#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Build English knowledge base index
"""

import os
from pathlib import Path
from langchain_community.document_loaders import DirectoryLoader, TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings

def build_english_index():
    """æ„å»ºè‹±æ–‡çŸ¥è¯†åº“ç´¢å¼•"""
    print("ğŸ”¨ Building English knowledge base index...")
    
    # è®¾ç½®è·¯å¾„
    kb_dir = Path("knowledge_base/problems_en")
    persist_dir = Path("chroma_db_en")
    
    if not kb_dir.exists():
        print(f"âŒ English knowledge base directory does not exist: {kb_dir}")
        return
    
    # åˆ é™¤æ—§çš„å‘é‡åº“ï¼ˆå¦‚æœå¯èƒ½çš„è¯ï¼‰
    if persist_dir.exists():
        import shutil
        try:
            shutil.rmtree(persist_dir)
            print("ğŸ—‘ï¸ Removed old vector store")
        except OSError:
            print("âš ï¸ Could not remove old vector store, will overwrite")
    
    try:
        # åŠ è½½è‹±æ–‡æ–‡æ¡£
        loader = DirectoryLoader(
            str(kb_dir),
            glob="**/*.md",
            loader_cls=TextLoader,
            loader_kwargs={"encoding": "utf-8"}
        )
        documents = loader.load()
        print(f"ğŸ“š Loaded {len(documents)} English documents")
        
        # æ˜¾ç¤ºæ–‡æ¡£å†…å®¹
        for i, doc in enumerate(documents):
            print(f"\nDocument {i+1}: {doc.metadata.get('source', 'Unknown')}")
            print(f"Content length: {len(doc.page_content)}")
            print(f"First 100 chars: {doc.page_content[:100]}...")
        
        # æ–‡æœ¬åˆ†å‰² - å¢åŠ åˆ‡ç‰‡å¤§å°ä»¥ä¿ç•™æ›´å¤šä¸Šä¸‹æ–‡
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1500,  # å¢åŠ åˆ°1500å­—ç¬¦ï¼Œä¿ç•™æ›´å¤šå®Œæ•´ä¿¡æ¯
            chunk_overlap=50,  # å¢åŠ é‡å ä»¥ä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
            separators=["\n\n", "\n", ". ", "! ", "? ", "; ", " "]
        )
        split_docs = text_splitter.split_documents(documents)
        print(f"ğŸ“„ Split into {len(split_docs)} document chunks")
        
        # ä½¿ç”¨æ›´ä¼˜ç§€çš„è‹±æ–‡åµŒå…¥æ¨¡å‹
        embeddings = SentenceTransformerEmbeddings(
            model_name="BAAI/bge-base-en-v1.5"  # ç›®å‰æœ€å¥½çš„è‹±æ–‡æ£€ç´¢æ¨¡å‹ä¹‹ä¸€
            # å…¶ä»–é€‰æ‹©:
            # "sentence-transformers/all-mpnet-base-v2"  # ç»å…¸é«˜è´¨é‡æ¨¡å‹
            # "sentence-transformers/all-MiniLM-L12-v2"  # å¹³è¡¡å‡†ç¡®æ€§å’Œé€Ÿåº¦
        )
        
        # åˆ›å»ºå‘é‡åº“
        vector_store = Chroma.from_documents(
            documents=split_docs,
            embedding=embeddings,
            collection_name="mixing_kb_en",
            persist_directory=str(persist_dir)
        )
        
        print("âœ… English knowledge base index built successfully!")
        
        # æµ‹è¯•æ£€ç´¢
        print("\nğŸ” Testing retrieval:")
        test_queries = ["RGB color conversion", "multi-component mixing", "dye preparation"]
        
        for query in test_queries:
            results = vector_store.similarity_search_with_score(query, k=3)
            print(f"\nQuery: '{query}'")
            for i, (result, score) in enumerate(results):
                print(f"  Result {i+1} (score: {score:.4f}): {result.page_content[:50]}...")
                print(f"  Source: {result.metadata.get('source', 'Unknown')}")
        
        return vector_store
        
    except Exception as e:
        print(f"âŒ Build failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    build_english_index()