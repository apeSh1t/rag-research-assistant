from pathlib import Path
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter

class KnowledgeBase:
    def __init__(self, kb_dir: str = "knowledge_base", use_english: bool = True):
        self.kb_dir = Path(kb_dir)
        self.use_english = use_english
        self.embedding_model = SentenceTransformerEmbeddings(
            model_name="BAAI/bge-base-en-v1.5"  # ä½¿ç”¨æ›´ä¼˜ç§€çš„è‹±æ–‡æ£€ç´¢æ¨¡å‹
        )
        self.vector_store = None
        self._load_vector_store()
    
    def _load_vector_store(self):
        # æ ¹æ®è¯­è¨€é€‰æ‹©ä¸åŒçš„å‘é‡åº“ç›®å½•
        if self.use_english:
            persist_dir = self.kb_dir.parent / "chroma_db_en"
            collection_name = "mixing_kb_en"
        else:
            persist_dir = self.kb_dir.parent / "chroma_db"
            collection_name = "mixing_kb"
        
        if not persist_dir.exists():
            raise FileNotFoundError(
                f"Vector store does not exist: {persist_dir}\n"
                "Please run: python knowledge_base/build_index.py first"
            )
        
        self.vector_store = Chroma(
            collection_name=collection_name,
            embedding_function=self.embedding_model,
            persist_directory=str(persist_dir)
        )
    
    def retrieve(self, query: str, k: int = 3) -> str:
        """æ£€ç´¢çŸ¥è¯†åº“"""
        if self.vector_store is None:
            return "çŸ¥è¯†åº“æœªåŠ è½½" if not self.use_english else "Knowledge base not loaded"
        
        results = self.vector_store.similarity_search_with_score(query, k=k)
        
        if not results:
            return f"æœªæ‰¾åˆ°ä¸'{query}'ç›¸å…³çš„ä¿¡æ¯" if not self.use_english else f"No information found for '{query}'"
        
        # æ˜¾ç¤ºè°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” Query: '{query}' - Search results:")
        for i, (doc, score) in enumerate(results):
            title = doc.metadata.get('title', 'Unknown')
            print(f"  {i+1}. Score: {score:.4f} - {title}")
        
        # æ·»åŠ ç›¸ä¼¼åº¦è¿‡æ»¤ï¼Œåªè¿”å›é«˜è´¨é‡åŒ¹é…
        formatted = "\n\n".join([
            f"ã€{doc.metadata.get('title', 'æœªå‘½å')}ã€‘(Score: {score:.4f})\n{doc.page_content}"
            for doc, score in results if score < 1.0  # è°ƒæ•´é˜ˆå€¼ä»¥è¿‡æ»¤ä½è´¨é‡åŒ¹é…
        ])
        
        return formatted if formatted else f"æœªæ‰¾åˆ°ä¸'{query}'é«˜åº¦ç›¸å…³çš„ä¿¡æ¯"
    
    def list_documents(self):
        """åˆ—å‡ºæ‰€æœ‰æ–‡æ¡£"""
        if self.vector_store is None:
            return "çŸ¥è¯†åº“æœªåŠ è½½"
        
        try:
            # è·å–æ‰€æœ‰æ–‡æ¡£ä¿¡æ¯
            all_docs = self.vector_store.get()
            if not all_docs['metadatas']:
                return "çŸ¥è¯†åº“ä¸­æ²¡æœ‰æ–‡æ¡£"
            
            titles = [meta.get('title', 'Unknown') for meta in all_docs['metadatas']]
            return f"çŸ¥è¯†åº“åŒ…å« {len(titles)} ä¸ªæ–‡æ¡£:\n" + "\n".join(f"- {title}" for title in titles)
        except Exception as e:
            return f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}"
    
    def add_document(self, file_path: str, title: str = None):
        """
        é€šç”¨æ–‡æ¡£æ·»åŠ æ–¹æ³•ï¼Œæ”¯æŒ PDF, DOCX, TXT, MD
        """
        if self.vector_store is None:
            return {"success": False, "message": "Knowledge base not loaded"}
        
        file_path_obj = Path(file_path)
        file_ext = file_path_obj.suffix.lower()
        doc_title = title or file_path_obj.name
        
        print(f"  [KB] æ­£åœ¨å¤„ç†æ–‡æ¡£: {doc_title} ({file_ext})")
        
        try:
            documents = []
            if file_ext == '.pdf':
                print(f"  [KB] æ­£åœ¨åŠ è½½ PDF å†…å®¹...")
                try:
                    from langchain_community.document_loaders import PyPDFLoader
                    loader = PyPDFLoader(str(file_path))
                    documents = loader.load()
                except Exception:
                    import PyPDF2
                    with open(file_path, "rb") as f:
                        reader = PyPDF2.PdfReader(f)
                        for page in reader.pages:
                            text = page.extract_text()
                            if text:
                                from langchain_core.documents import Document
                                documents.append(Document(page_content=text))
            
            elif file_ext == '.docx':
                print(f"  [KB] æ­£åœ¨åŠ è½½ Word å†…å®¹...")
                from langchain_community.document_loaders import Docx2txtLoader
                loader = Docx2txtLoader(str(file_path))
                documents = loader.load()
                
            elif file_ext in ['.txt', '.md']:
                print(f"  [KB] æ­£åœ¨åŠ è½½æ–‡æœ¬å†…å®¹...")
                from langchain_community.document_loaders import TextLoader
                loader = TextLoader(str(file_path), encoding='utf-8')
                documents = loader.load()
            
            if not documents:
                return {"success": False, "message": f"æœªèƒ½ä»æ–‡ä»¶ {file_ext} ä¸­æå–å†…å®¹"}

            print(f"  [KB] å†…å®¹æå–å®Œæˆ, æ­£åœ¨è¿›è¡Œæ–‡æœ¬åˆ‡åˆ†...")
            # æ·»åŠ å…ƒæ•°æ®
            for doc in documents:
                doc.metadata['title'] = doc_title
                doc.metadata['source'] = str(file_path_obj.name)
            
            # åˆ†å‰²æ–‡æœ¬
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=100,
                separators=["\n\n", "\n", ". ", " "]
            )
            split_docs = text_splitter.split_documents(documents)
            
            print(f"  [KB] åˆ‡åˆ†å®Œæˆ, å…±æœ‰ {len(split_docs)} ä¸ªç‰‡æ®µã€‚æ­£åœ¨è°ƒç”¨ Embedding æ¨¡å‹å†™å…¥å‘é‡åº“...")
            print(f"  [KB] æ³¨æ„: å¦‚æœæ˜¯é¦–æ¬¡è¿è¡Œ, æ¨¡å‹åŠ è½½å¯èƒ½éœ€è¦ 1-2 åˆ†é’Ÿ...")
            
            # æ·»åŠ åˆ°å‘é‡åº“
            self.vector_store.add_documents(split_docs)
            
            print(f"  [KB] å‘é‡åº“å†™å…¥æˆåŠŸ!")
            return {
                "success": True,
                "message": f"æˆåŠŸç´¢å¼• {len(split_docs)} ä¸ªç‰‡æ®µ",
                "chunks": len(split_docs),
                "title": doc_title
            }
            
        except Exception as e:
            return {"success": False, "message": f"ç´¢å¼•å¤±è´¥: {str(e)}"}

    def add_pdf_document(self, pdf_path: str, title: str = None):
        """ä¿æŒå‘åå…¼å®¹"""
        return self.add_document(pdf_path, title)

    def delete_document(self, title: str):
        """ä»å‘é‡åº“ä¸­åˆ é™¤æ–‡æ¡£"""
        if self.vector_store is None:
            return False
        try:
            # 1. æŸ¥æ‰¾å…·æœ‰è¯¥ title çš„æ‰€æœ‰æ–‡æ¡£çš„ ID
            # Chroma çš„ get æ–¹æ³•æ”¯æŒ where è¿‡æ»¤
            results = self.vector_store.get(where={"title": title})
            ids = results.get("ids", [])
            
            if ids:
                # 2. æŒ‰ ID åˆ é™¤
                self.vector_store.delete(ids=ids)
                return True
            return False
        except Exception as e:
            print(f"Error deleting document from vector store: {e}")
            return False